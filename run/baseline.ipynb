{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../results/baselines/rel-f1/rel-f1-qualifying_baseline.log\n",
      "../results/baselines/rel-f1/rel-f1-dnf_baseline.log\n",
      "../results/baselines/rel-f1/rel-f1-position_baseline.log\n",
      "../results/baselines/rel-stackex/rel-stackex-badges_baseline.log\n",
      "../results/baselines/rel-stackex/rel-stackex-engage_baseline.log\n",
      "../results/baselines/rel-stackex/rel-stackex-votes_baseline.log\n",
      "../results/baselines/rel-amazon/rel-amazon-ltv_baseline.log\n",
      "../results/baselines/rel-amazon/rel-amazon-product-churn_baseline.log\n",
      "../results/baselines/rel-amazon/rel-amazon-churn_baseline.log\n",
      "../results/baselines/rel-amazon/rel-amazon-product-ltv_baseline.log\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# Define the pattern to match log files\n",
    "pattern = \"../results/baselines/*/*_baseline.log\"\n",
    "#pattern = \"../results/baselines/*/*_lgbm.log\"\n",
    "\n",
    "# Use glob to find all matching log files\n",
    "log_files = glob.glob(pattern)\n",
    "\n",
    "# Print the list of log files\n",
    "for log_file in log_files:\n",
    "    print(log_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from statistics import mean, stdev\n",
    "\n",
    "\n",
    "def whole_file():\n",
    "    # Initialize lists to store the computed statistics for each file\n",
    "    val_metrics_list = []\n",
    "    test_metrics_list = []\n",
    "    file_names = []\n",
    "    files_without_matches = []\n",
    "\n",
    "    # Iterate through each log file\n",
    "    for log_file in log_files:\n",
    "        with open(log_file, 'r') as f:\n",
    "            content = f.read()\n",
    "\n",
    "        # Use regular expressions to extract metrics for all occurrences\n",
    "        val_matches = re.findall(r'majority:\\nVal: (.+)', content)\n",
    "        test_matches = re.findall(r'Test: (.+)', content)\n",
    "\n",
    "        # Check if there are any matches\n",
    "        if not val_matches and not test_matches:\n",
    "            files_without_matches.append(log_file)\n",
    "            continue\n",
    "\n",
    "\n",
    "        val_metrics = []\n",
    "        test_metrics = []\n",
    "\n",
    "        # Parse and append metrics for validation\n",
    "        for match in val_matches:\n",
    "            val_metrics.append(eval(match))\n",
    "\n",
    "        # Parse and append metrics for test\n",
    "        for match in test_matches:\n",
    "            test_metrics.append(eval(match))\n",
    "\n",
    "        # Compute mean and standard deviation for val metrics\n",
    "        if val_metrics:\n",
    "            val_means = {k: mean([d[k] for d in val_metrics]) for k in val_metrics[0]}\n",
    "            val_stdevs = {k: stdev([d[k] for d in val_metrics]) for k in val_metrics[0]}\n",
    "            val_metrics_list.append((val_means, val_stdevs))\n",
    "\n",
    "        # Compute mean and standard deviation for test metrics\n",
    "        if test_metrics:\n",
    "            test_means = {k: mean([d[k] for d in test_metrics]) for k in test_metrics[0]}\n",
    "            test_stdevs = {k: stdev([d[k] for d in test_metrics]) for k in test_metrics[0]}\n",
    "            test_metrics_list.append((test_means, test_stdevs))\n",
    "\n",
    "        # Store the filename\n",
    "        file_names.append(os.path.basename(log_file))\n",
    "\n",
    "    # Print files without matches\n",
    "    if files_without_matches:\n",
    "        print(\"Log files without exact matches:\")\n",
    "        for file in files_without_matches:\n",
    "            print(file)\n",
    "\n",
    "\n",
    "    # Create dataframes to store the computed statistics for each file\n",
    "    df_val = pd.DataFrame(val_metrics_list, columns=['mean', 'std'])\n",
    "    df_val['file_name'] = file_names\n",
    "    df_test = pd.DataFrame(test_metrics_list, columns=['mean', 'std'])\n",
    "    df_test['file_name'] = file_names\n",
    "    \n",
    "    return df_test, df_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rel-f1-dnf_lgbm.log\n",
      "\n",
      "Test Metrics:\n",
      "{'average_precision': 0.4284579341878245, 'accuracy': 0.7051282051282052, 'f1': 0.0, 'roc_auc': 0.6753828136436832}\n",
      "{'average_precision': 0.03549766656635324, 'accuracy': 0.0, 'f1': 0.0, 'roc_auc': 0.04028106749475347}\n"
     ]
    }
   ],
   "source": [
    "# Display the entire dataframe as a string\n",
    "idx=0\n",
    "print(df_val['file_name'].iloc[idx])\n",
    "print(\"\\nTest Metrics:\")\n",
    "print(df_test.iloc[idx]['mean'])\n",
    "print(df_test.iloc[idx]['std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "from statistics import mean\n",
    "\n",
    "def collect_average_performance(log_file, methods):\n",
    "    performance = defaultdict(list)\n",
    "    current_method = None\n",
    "\n",
    "    with open(log_file, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line in methods:\n",
    "                current_method = line[:-1]  # Remove the colon\n",
    "            elif line.startswith(\"Test:\"):\n",
    "                match = re.search(r'{.*}', line)\n",
    "                if match:\n",
    "                    test_metrics = eval(match.group())\n",
    "                    performance[current_method].append(test_metrics)\n",
    "            ##elif line.startswith(\"Val:\"):\n",
    "            #    match = re.search(r'{.*}', line)\n",
    "            #    if match:\n",
    "            #        val_metrics = eval(match.group())\n",
    "            #        performance[current_method].append(val_metrics)\n",
    "\n",
    "    average_performance = {}\n",
    "    for method, metrics_list in performance.items():\n",
    "        if metrics_list:\n",
    "            num_seeds = len(metrics_list)\n",
    "            average_metrics = {k: mean([d[k] for d in metrics_list]) for k in metrics_list[0]}\n",
    "            std_metrics = {k: stdev([d[k] for d in metrics_list]) for k in metrics_list[0]}\n",
    "            average_performance[method] = {\"average_metrics\": average_metrics, \"std_metrics\": std_metrics, \"num_seeds\": num_seeds}\n",
    "\n",
    "    return average_performance\n",
    "\n",
    "\n",
    "def collect_avg(log_files, methods):\n",
    "    for log_file in log_files:\n",
    "        print(f\"File: {log_file}\")\n",
    "        average_performance = collect_average_performance(log_file, methods)\n",
    "\n",
    "\n",
    "        for method, data in average_performance.items():\n",
    "            print(f\"Method: {method}\")\n",
    "            print(f\"Average Metrics over {data['num_seeds']} seeds:\")\n",
    "            for metric, value in data['average_metrics'].items():\n",
    "                print(f\"{metric}: {value}\")\n",
    "            print(f\"Std Metrics over {data['num_seeds']} seeds:\")\n",
    "\n",
    "            for metric, value in data['std_metrics'].items():\n",
    "                print(f\"{metric}: {value}\")\n",
    "        \n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../results/baselines/rel-f1/rel-f1-qualifying_baseline.log',\n",
       " '../results/baselines/rel-f1/rel-f1-dnf_baseline.log',\n",
       " '../results/baselines/rel-f1/rel-f1-position_baseline.log',\n",
       " '../results/baselines/rel-stackex/rel-stackex-badges_baseline.log',\n",
       " '../results/baselines/rel-stackex/rel-stackex-engage_baseline.log',\n",
       " '../results/baselines/rel-stackex/rel-stackex-votes_baseline.log',\n",
       " '../results/baselines/rel-amazon/rel-amazon-ltv_baseline.log',\n",
       " '../results/baselines/rel-amazon/rel-amazon-product-churn_baseline.log',\n",
       " '../results/baselines/rel-amazon/rel-amazon-churn_baseline.log',\n",
       " '../results/baselines/rel-amazon/rel-amazon-product-ltv_baseline.log']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: ../results/baselines/rel-f1/rel-f1-position_baseline.log\n",
      "Method: global_zero\n",
      "Average Metrics over 5 seeds:\n",
      "mae: 11.926206140350878\n",
      "rmse: 13.014703602787927\n",
      "Std Metrics over 5 seeds:\n",
      "mae: 0.0\n",
      "rmse: 0.0\n",
      "\n",
      "Method: global_mean\n",
      "Average Metrics over 5 seeds:\n",
      "mae: 4.512880528786756\n",
      "rmse: 5.512014024135418\n",
      "Std Metrics over 5 seeds:\n",
      "mae: 0.0\n",
      "rmse: 0.0\n",
      "\n",
      "Method: global_median\n",
      "Average Metrics over 5 seeds:\n",
      "mae: 4.399100877192982\n",
      "rmse: 5.319882538110992\n",
      "Std Metrics over 5 seeds:\n",
      "mae: 0.0\n",
      "rmse: 0.0\n",
      "\n",
      "Method: entity_mean\n",
      "Average Metrics over 5 seeds:\n",
      "mae: 8.501357509542604\n",
      "rmse: 10.211046868024601\n",
      "Std Metrics over 5 seeds:\n",
      "mae: 0.0\n",
      "rmse: 0.0\n",
      "\n",
      "Method: entity_median\n",
      "Average Metrics over 5 seeds:\n",
      "mae: 8.518508771929824\n",
      "rmse: 10.22265948741393\n",
      "Std Metrics over 5 seeds:\n",
      "mae: 0.0\n",
      "rmse: 0.0\n",
      "\n",
      "File: ../results/baselines/rel-stackex/rel-stackex-votes_baseline.log\n",
      "Method: global_zero\n",
      "Average Metrics over 5 seeds:\n",
      "mae: 0.4222357569467319\n",
      "rmse: 1.7800068692241051\n",
      "Std Metrics over 5 seeds:\n",
      "mae: 0.0\n",
      "rmse: 0.0\n",
      "\n",
      "Method: global_mean\n",
      "Average Metrics over 5 seeds:\n",
      "mae: 0.8286966055083371\n",
      "rmse: 1.7598734353556842\n",
      "Std Metrics over 5 seeds:\n",
      "mae: 0.0\n",
      "rmse: 0.0\n",
      "\n",
      "Method: global_median\n",
      "Average Metrics over 5 seeds:\n",
      "mae: 0.4222357569467319\n",
      "rmse: 1.7800068692241051\n",
      "Std Metrics over 5 seeds:\n",
      "mae: 0.0\n",
      "rmse: 0.0\n",
      "\n",
      "Method: entity_mean\n",
      "Average Metrics over 5 seeds:\n",
      "mae: 0.4917811766509429\n",
      "rmse: 1.3904079524880315\n",
      "Std Metrics over 5 seeds:\n",
      "mae: 0.0\n",
      "rmse: 0.0\n",
      "\n",
      "Method: entity_median\n",
      "Average Metrics over 5 seeds:\n",
      "mae: 0.4745716363274768\n",
      "rmse: 1.426501357715838\n",
      "Std Metrics over 5 seeds:\n",
      "mae: 0.0\n",
      "rmse: 0.0\n",
      "\n",
      "File: ../results/baselines/rel-amazon/rel-amazon-ltv_baseline.log\n",
      "Method: global_zero\n",
      "Average Metrics over 5 seeds:\n",
      "mae: 3.848652912136346\n",
      "rmse: 13.363510130983899\n",
      "Std Metrics over 5 seeds:\n",
      "mae: 0.0\n",
      "rmse: 0.0\n",
      "\n",
      "Method: global_mean\n",
      "Average Metrics over 5 seeds:\n",
      "mae: 4.2549111024012785\n",
      "rmse: 12.799401856334859\n",
      "Std Metrics over 5 seeds:\n",
      "mae: 0.0\n",
      "rmse: 0.0\n",
      "\n",
      "Method: global_median\n",
      "Average Metrics over 5 seeds:\n",
      "mae: 3.488099071226107\n",
      "rmse: 13.110533825769132\n",
      "Std Metrics over 5 seeds:\n",
      "mae: 0.0\n",
      "rmse: 0.0\n",
      "\n",
      "Method: entity_mean\n",
      "Average Metrics over 5 seeds:\n",
      "mae: 4.217407863114104\n",
      "rmse: 11.957251330599524\n",
      "Std Metrics over 5 seeds:\n",
      "mae: 0.0\n",
      "rmse: 0.0\n",
      "\n",
      "Method: entity_median\n",
      "Average Metrics over 5 seeds:\n",
      "mae: 4.21626285575539\n",
      "rmse: 12.07488960772671\n",
      "Std Metrics over 5 seeds:\n",
      "mae: 0.0\n",
      "rmse: 0.0\n",
      "\n",
      "File: ../results/baselines/rel-amazon/rel-amazon-product-ltv_baseline.log\n",
      "Method: global_zero\n",
      "Average Metrics over 5 seeds:\n",
      "mae: 17.388250066248784\n",
      "rmse: 78.13745934390502\n",
      "Std Metrics over 5 seeds:\n",
      "mae: 0.0\n",
      "rmse: 0.0\n",
      "\n",
      "Method: global_mean\n",
      "Average Metrics over 5 seeds:\n",
      "mae: 17.428525642387115\n",
      "rmse: 76.42806955222487\n",
      "Std Metrics over 5 seeds:\n",
      "mae: 0.0\n",
      "rmse: 0.0\n",
      "\n",
      "Method: global_median\n",
      "Average Metrics over 5 seeds:\n",
      "mae: 15.473041079198874\n",
      "rmse: 77.52504790274507\n",
      "Std Metrics over 5 seeds:\n",
      "mae: 0.0\n",
      "rmse: 0.0\n",
      "\n",
      "Method: entity_mean\n",
      "Average Metrics over 5 seeds:\n",
      "mae: 19.14986508579242\n",
      "rmse: 91.03813229902008\n",
      "Std Metrics over 5 seeds:\n",
      "mae: 0.0\n",
      "rmse: 0.0\n",
      "\n",
      "Method: entity_median\n",
      "Average Metrics over 5 seeds:\n",
      "mae: 19.423317342817022\n",
      "rmse: 91.81831002551297\n",
      "Std Metrics over 5 seeds:\n",
      "mae: 0.0\n",
      "rmse: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classif_idx = [2,5,6,9]\n",
    "\n",
    "\n",
    "#methods =  (\"random:\", \"majority:\")\n",
    "methods = (\"entity_median:\", \"entity_mean:\", \"global_mean:\", \"global_median:\", \"global_zero:\")\n",
    "collect_avg([log_files[i] for i in classif_idx], methods)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
